{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from collections.abc import Iterable\n",
    "import time\n",
    "import math\n",
    "import PyFiles.MLFunctionsForPytorch as MLFun\n",
    "import pandas as pd\n",
    "batchSize = 128 #Batch size of training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define our neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "  '''\n",
    "    Multilayer Perceptron for regression.\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.norm0 = nn.BatchNorm1d(3)\n",
    "    self.linear1 = nn.Linear(in_features=3, out_features=64)\n",
    "    self.norm1 = nn.BatchNorm1d(64)\n",
    "    self.act1 = nn.LeakyReLU()\n",
    "    self.linear2 = nn.Linear(in_features=64, out_features=16)\n",
    "    self.norm2 = nn.BatchNorm1d(16)\n",
    "    self.act2 = nn.LeakyReLU()\n",
    "    self.output = nn.Linear(in_features=16, out_features = 3)\n",
    "    \n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''\n",
    "      Forward pass\n",
    "    '''\n",
    "    x = self.norm0(x)\n",
    "    x = self.linear1(x)\n",
    "    x = self.norm1(x)\n",
    "    x = self.act1(x)\n",
    "    x = self.linear2(x)\n",
    "    x = self.norm2(x)\n",
    "    x = self.act2(x)\n",
    "    x = self.output(x)\n",
    "    \n",
    "    \n",
    "    return x\n",
    "\n",
    "class MLP2Input(nn.Module):\n",
    "  '''\n",
    "    Multilayer Perceptron for regression.\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.norm0 = nn.BatchNorm1d(2)\n",
    "    self.linear1 = nn.Linear(in_features=2, out_features=64)\n",
    "    self.norm1 = nn.BatchNorm1d(64)\n",
    "    self.act1 = nn.LeakyReLU()\n",
    "    self.linear2 = nn.Linear(in_features=64, out_features=16)\n",
    "    self.norm2 = nn.BatchNorm1d(16)\n",
    "    self.act2 = nn.LeakyReLU()\n",
    "    self.output = nn.Linear(in_features=16, out_features = 3)\n",
    "    \n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''\n",
    "      Forward pass\n",
    "    '''\n",
    "    x = self.norm0(x)\n",
    "    x = self.linear1(x)\n",
    "    x = self.norm1(x)\n",
    "    x = self.act1(x)\n",
    "    x = self.linear2(x)\n",
    "    x = self.norm2(x)\n",
    "    x = self.act2(x)\n",
    "    x = self.output(x)\n",
    "    \n",
    "    \n",
    "    return x\n",
    "\n",
    "class MLP3Input(nn.Module):\n",
    "    '''\n",
    "    Multilayer Perceptron for regression.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.norm0 = nn.BatchNorm1d(3)\n",
    "        self.linear1 = nn.Linear(in_features=3, out_features=256)\n",
    "        self.norm1 = nn.BatchNorm1d(256)\n",
    "        self.act1 = nn.LeakyReLU()\n",
    "        self.linear2 = nn.Linear(in_features=256, out_features=128)\n",
    "        self.norm2 = nn.BatchNorm1d(128)\n",
    "        self.act2 = nn.LeakyReLU()\n",
    "        self.linear3 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.norm3 = nn.BatchNorm1d(64)\n",
    "        self.act3 = nn.LeakyReLU()\n",
    "        self.linear4 = nn.Linear(in_features=64, out_features=32)\n",
    "        self.norm4 = nn.BatchNorm1d(32)\n",
    "        self.act4 = nn.LeakyReLU()\n",
    "        self.output = nn.Linear(in_features=32, out_features = 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Forward pass\n",
    "        '''\n",
    "        x = self.norm0(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.linear3(x)\n",
    "        x = self.norm3(x)\n",
    "        x = self.act3(x)\n",
    "        x = self.linear4(x)\n",
    "        x = self.norm4(x)\n",
    "        x = self.act4(x)\n",
    "        x = self.output(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeOutputRescaleLog(MLFun.ProcessFunction):\n",
    "    def __init__(self, firstScale, secondScale, thirdScale):\n",
    "        self.firstScale = firstScale\n",
    "        self.secondScale = secondScale\n",
    "        self.thirdScale = thirdScale\n",
    "    \n",
    "    def process(self, input):\n",
    "        #Process each column by their respective scales\n",
    "        \n",
    "        colOne = input[:, 0]\n",
    "        colOne = colOne / self.firstScale\n",
    "        colOne = np.log(colOne)\n",
    "        \n",
    "        colTwo = input[:, 1]\n",
    "        colTwo = colTwo / self.secondScale\n",
    "        colTwo = np.log(colTwo)\n",
    "        \n",
    "        colThree = input[:, 2]\n",
    "        colThree = colThree / self.thirdScale\n",
    "        colThree = np.log(colThree)\n",
    "        \n",
    "        output = torch.stack((colOne, colTwo, colThree), axis = -1)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def invertProcess(self, input):\n",
    "        \n",
    "        colOne = input[:, 0]\n",
    "        colOne = np.exp(colOne)\n",
    "        colOne = colOne * self.firstScale\n",
    "        colOne = torch.Tensor(colOne)\n",
    "        \n",
    "        colTwo = input[:, 1]\n",
    "        colTwo = np.exp(colTwo)\n",
    "        colTwo = colTwo * self.secondScale\n",
    "        colTwo = torch.Tensor(colTwo)\n",
    "        \n",
    "        colThree = input[:, 2]\n",
    "        colThree = np.exp(colThree)\n",
    "        colThree = colThree * self.thirdScale\n",
    "        colThree = torch.Tensor(colThree)\n",
    "        \n",
    "        output = torch.stack((colOne, colTwo, colThree), axis = -1)\n",
    "        output = output.cpu().detach().numpy()\n",
    "        \n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "numPoints = 20000\n",
    "#numPoints = 500\n",
    "#numPoints = 20000\n",
    "\n",
    "#filename = 'Data_Fuchs_v_2.2_lambda_um_0.8_points_' + str(numPoints) + '_seed_0.h5'\n",
    "#filename = 'Dataset/Data_Fuchs_v_2.2_Wright_Pat_Narrow_Range_lambda_um_0.8_points_' + str(numPoints) + '_seed_0.h5'\n",
    "#filename = 'Dataset/Data_Fuchs_v_2.3_energy_limit_0.01_lambda_um_0.8_points_' + str(numPoints) + '_seed_0.h5'\n",
    "\n",
    "# filename = 'Dataset/Data_Fuchs_v_2.7_Wright_Pat_Narrow_Range_with_Focal_Dist_energy_limit_0.01_deviation_0.0_lambda_um_0.8_points_' \\\n",
    "#                  + str(numPoints) + '_seed_0.h5'\n",
    "# filename = 'Dataset/Data_Fuchs_v_2.8_Wright_Pat_Narrow_Range_with_Focal_Dist_energy_limit_0.01_deviation_0.0_' \\\n",
    "#             + 'lambda_um_0.8_points_' + str(numPoints) + '_seed_0.h5'\n",
    "#filename = 'Dataset/Data_Fuchs_v_2.91_Wright_Pat_intens_focalD_thick_scan_lambda_um_0.8_points_' + str(numPoints) + '_seed_3.h5'\n",
    "#filename = 'Dataset/Data_Fuchs_v_2.9_Wright_Pat_Narrow_Range_with_Focal_Dist_energy_limit_0.01_deviation_0.0_lambda_um_0.8_points_20000_seed_0.h5'\n",
    "filename = 'datasets/Energy/fuchs_v3_points_20000_noise_10.h5'\n",
    "\n",
    "\n",
    "h5File = h5py.File(filename, 'r+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.00246809e+18  9.65419743e+00  8.91527001e+00  2.06861198e-02\n",
      "   1.49958621e+06  6.98367217e-03]\n",
      " [ 2.66931281e+18  1.62030588e+00 -3.32389871e+00  2.76562530e-01\n",
      "   3.45491626e+07  5.30666304e-02]\n",
      " [ 5.23459779e+18  8.88063276e-01  9.85435536e+00  7.25453301e-01\n",
      "   1.91786607e+08  1.31634819e-01]\n",
      " ...\n",
      " [ 1.67512102e+18  2.77970861e+00  8.53217179e+00  7.25321819e-02\n",
      "   4.34108480e+06  1.53091706e-02]\n",
      " [ 4.22902816e+18  5.47370265e-01 -5.26398692e+00  7.39598795e-01\n",
      "   1.45924577e+08  1.41954341e-01]\n",
      " [ 2.98502865e+18  9.71667769e+00  5.01767066e+00  5.96168289e-02\n",
      "   6.25591344e+06  1.52332788e-02]]\n"
     ]
    }
   ],
   "source": [
    "#Read columns\n",
    "\n",
    "intens = h5File['Intensity_(W_cm2)']\n",
    "duration = h5File['Pulse_Duration_(fs)']\n",
    "thickness = h5File['Target_Thickness (um)']\n",
    "spotSize = h5File['Spot_Size_(FWHM um)']\n",
    "focalDist = h5File['Focal_Distance_(um)']\n",
    "maxEnergy = h5File['Max_Proton_Energy_(MeV)']\n",
    "totalEnergy = h5File['Total_Proton_Energy_(MeV)']\n",
    "avgEnergy = h5File['Avg_Proton_Energy_(MeV)']\n",
    "\n",
    "\n",
    "#Convert columns into numpy arrays\n",
    "npIntens = np.fromiter(intens, float)\n",
    "npDuration = np.fromiter(duration, float)\n",
    "npThickness = np.fromiter(thickness, float)\n",
    "npSpot = np.fromiter(spotSize, float)\n",
    "npDist = np.fromiter(focalDist, float)\n",
    "npMaxEnergy = np.fromiter(maxEnergy, float)\n",
    "npTotalEnergy = np.fromiter(totalEnergy, float)\n",
    "npAvgEnergy = np.fromiter(avgEnergy, float)\n",
    "\n",
    "\n",
    "# #Join all of those arrays into one big numpy array\n",
    "# npFile = np.dstack((npIntens, npDuration, npThickness, npSpot, npMaxEnergy, npTotalEnergy, npAvgEnergy))\n",
    "# npFile = npFile.reshape(numPoints, 7)\n",
    "\n",
    "# npTrain = npFile[:math.floor(.9*numPoints), 0:7]\n",
    "# #npTest = npFile[math.floor(.9*numPoints):, 0:7]\n",
    "\n",
    "# npTrain = npFile[:, 0:7]\n",
    "\n",
    "# #Two input version\n",
    "# npFile = np.dstack((npIntens, npThickness, npMaxEnergy, npTotalEnergy, npAvgEnergy))\n",
    "# npFile = npFile.reshape(numPoints, 5)\n",
    "# npTrain = npFile[:math.floor(.9*numPoints), 0:5]\n",
    "# npTest = npFile[math.floor(.9*numPoints):, 0:5]\n",
    "\n",
    "#Three input version\n",
    "npFile = np.dstack((npIntens, npThickness, npDist, npMaxEnergy, npTotalEnergy, npAvgEnergy))\n",
    "npFile = npFile.reshape(numPoints, 6)\n",
    "\n",
    "#If I want to take a subset of the dataset, do it in the line below\n",
    "numPoints = 20000\n",
    "np.random.shuffle(npFile)\n",
    "npFile = npFile[:numPoints]\n",
    "print(npFile)\n",
    "\n",
    "npTrain = npFile[:math.floor(.9*numPoints), 0:6]\n",
    "npTest = npFile[math.floor(.9*numPoints):, 0:6]\n",
    "\n",
    "# print(npTrain)\n",
    "\n",
    "#npTrain = npFile[:, 0:5]\n",
    "#npTrain = npFile[:, 0:6]\n",
    "\n",
    "#print(npFile.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check to see if there's any negative values that are not focal distance\n",
    "for index in range(numPoints):\n",
    "    if(npFile[index, 0] < 0):\n",
    "        print(index)\n",
    "        print(npFile[index, 0])\n",
    "        \n",
    "    elif(npFile[index, 3] < 0):\n",
    "        print(index)\n",
    "        print(npFile[index, 3])\n",
    "        \n",
    "    elif(npFile[index, 4] < 0):\n",
    "        print(index)\n",
    "        print(npFile[index, 4])\n",
    "        \n",
    "    elif(npFile[index, 5] < 0):\n",
    "        print(index)\n",
    "        print(npFile[index, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #filename_test = 'Data_Fuchs_v_2.2_Wright_Pat_Narrow_Range_lambda_um_0.8_points_' + str(numPoints) + '_seed_0.h5'\n",
    "# #filename_test = 'Dataset/Data_Fuchs_v_2.2_Wright_Pat_Narrow_Range_lambda_um_0.8_points_' + str(100000) + '_seed_1.h5'\n",
    "# #filename_test = 'Data_Fuchs_v_2.3_energy_limit_0.01_lambda_um_0.8_points_' + str(numPoints) + '_seed_0.h5'\n",
    "# filename_test = 'Dataset/Data_Fuchs_v_2.7_Wright_Pat_Narrow_Range_with_Focal_Dist_energy_limit_0.01_deviation_0.0_lambda_um_0.8_points_100000_seed_3.h5'\n",
    "\n",
    "# h5FileTest = h5py.File(filename_test, 'r+')\n",
    "\n",
    "# #Read columns\n",
    "\n",
    "# intens = h5FileTest['Intensity_(W_cm2)']\n",
    "# duration = h5FileTest['Pulse_Duration_(fs)']\n",
    "# thickness = h5FileTest['Target_Thickness (um)']\n",
    "# spotSize = h5FileTest['Spot_Size_(FWHM um)']\n",
    "# focalDist = h5FileTest['Focal_Distance_(um)']\n",
    "# maxEnergy = h5FileTest['Max_Proton_Energy_(MeV)']\n",
    "# totalEnergy = h5FileTest['Total_Proton_Energy_(MeV)']\n",
    "# avgEnergy = h5FileTest['Avg_Proton_Energy_(MeV)']\n",
    "\n",
    "\n",
    "# #Convert columns into numpy arrays\n",
    "# npIntens = np.fromiter(intens, float)\n",
    "# npDuration = np.fromiter(duration, float)\n",
    "# npThickness = np.fromiter(thickness, float)\n",
    "# npSpot = np.fromiter(spotSize, float)\n",
    "# npDist = np.fromiter(focalDist, float)\n",
    "# npMaxEnergy = np.fromiter(maxEnergy, float)\n",
    "# npTotalEnergy = np.fromiter(totalEnergy, float)\n",
    "# npAvgEnergy = np.fromiter(avgEnergy, float)\n",
    "\n",
    "\n",
    "# # #Join all of those arrays into one big numpy array\n",
    "# # npFile = np.dstack((npIntens, npDuration, npThickness, npSpot, npMaxEnergy, npTotalEnergy, npAvgEnergy))\n",
    "\n",
    "# # npFile = npFile.reshape(100000, 7)\n",
    "\n",
    "# # #npTrain = npFile[:math.floor(.9*numPoints), 0:7]\n",
    "# # #npTest = npFile[math.floor(.9*numPoints):, 0:7]\n",
    "\n",
    "# # npTest = npFile[:, 0:7]\n",
    "\n",
    "# # #Two input version\n",
    "# # npFile = np.dstack((npIntens, npThickness, npMaxEnergy, npTotalEnergy, npAvgEnergy))\n",
    "# # npFile = npFile.reshape(100000, 5)\n",
    "\n",
    "# # npTest = npFile[:, 0:5]\n",
    "\n",
    "# #Three input version\n",
    "# npFile = np.dstack((npIntens, npThickness, npDist, npMaxEnergy, npTotalEnergy, npAvgEnergy))\n",
    "# npFile = npFile.reshape(100000, 6)\n",
    "\n",
    "# npTest = npFile[:, 0:6]\n",
    "\n",
    "# print(npFile.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = h5File.create_dataset(name=None, data=npTrain)\n",
    "test_dataset = h5File.create_dataset(name=None, data=npTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose our loss function\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List which epochs we should test\n",
    "\n",
    "#epochList = [1, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 100, 150, 200, 250]\n",
    "#epochList = [1, 5, 10, 15, 20, 25, 50, 75, 100, 150, 200]\n",
    "#epochList = [1, 5, 10, 15, 20, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]\n",
    "#epochList = [1, 5, 10, 15, 20, 25]\n",
    "#epochList = [1, 5, 10, 15, 20, 30, 35, 40, 45]\n",
    "#epochList = [1, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100]\n",
    "#epochList = [1, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 100]\n",
    "#epochList = [1, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "#epochList = [1,5, 10, 15, 20, 25, 30, 35]\n",
    "#epochList = [35]\n",
    "#epochList = [1, 5, 35]\n",
    "epochList = [1, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 60, 70, 80, 90, 100]\n",
    "#epochList = [1, 5, 20]\n",
    "#epochList = [5, 10, 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 1 epochs.\n",
      "Starting epoch 1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m targetProcess \u001b[38;5;241m=\u001b[39m ThreeOutputRescaleLog(maxScale, totalScale, avgScale)\n\u001b[1;32m     19\u001b[0m inputProcess \u001b[38;5;241m=\u001b[39m MLFun\u001b[38;5;241m.\u001b[39mLogFirstColRescaleFun(intensScale)\n\u001b[0;32m---> 21\u001b[0m modelMSE, modelPercentError, trainMSE, trainPercent, timeList, cpuTime \u001b[38;5;241m=\u001b[39m \u001b[43mMLFun\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetModelError\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDJINN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochList\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                                                                                    \u001b[49m\u001b[43mtraining_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                                                                                    \u001b[49m\u001b[43mnumInputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessInputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputProcess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                                                                                    \u001b[49m\u001b[43mprocessTargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtargetProcess\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/New Summer/PyFiles/MLFunctionsForPytorch.py:283\u001b[0m, in \u001b[0;36mgetModelError\u001b[0;34m(PTNetwork, epochList, loss_function, trainDataset, testDataset, numInputs, learningRate, batchSize, processInputs, processTargets)\u001b[0m\n\u001b[1;32m    280\u001b[0m startTimeProcess \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mprocess_time()\n\u001b[1;32m    282\u001b[0m \u001b[38;5;66;03m#First train the network\u001b[39;00m\n\u001b[0;32m--> 283\u001b[0m \u001b[43mtrainNetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumEpochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumInputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnumInputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessInputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprocessInputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessTargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprocessTargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;66;03m#End clock\u001b[39;00m\n\u001b[1;32m    286\u001b[0m endTimeProcess \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mprocess_time()\n",
      "File \u001b[0;32m~/New Summer/PyFiles/MLFunctionsForPytorch.py:204\u001b[0m, in \u001b[0;36mtrainNetwork\u001b[0;34m(model, loss_function, optimizer, numEpochs, dataloader, numInputs, processInputs, processTargets)\u001b[0m\n\u001b[1;32m    201\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m# Perform backward pass\u001b[39;00m\n\u001b[1;32m    207\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/loss.py:536\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 536\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/functional.py:3284\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, target):\n\u001b[1;32m   3281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   3282\u001b[0m         mse_loss, (\u001b[38;5;28minput\u001b[39m, target), \u001b[38;5;28minput\u001b[39m, target, size_average\u001b[38;5;241m=\u001b[39msize_average, reduce\u001b[38;5;241m=\u001b[39mreduce, reduction\u001b[38;5;241m=\u001b[39mreduction\n\u001b[1;32m   3283\u001b[0m     )\n\u001b[0;32m-> 3284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m()):\n\u001b[1;32m   3285\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   3286\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3287\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis will likely lead to incorrect results due to broadcasting. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3288\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()),\n\u001b[1;32m   3289\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   3290\u001b[0m     )\n\u001b[1;32m   3291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "#Initialize neural network and dataloader\n",
    "#model1Layer = MultiRegressor1Layer().to('cuda')\n",
    "#model = MLP().to('cuda')\n",
    "\n",
    "# modelMSE, modelPercentError, trainMSE, trainPercent, timeList = MLFun.getModelError(model, epochList, loss_function, \n",
    "#                                                                                     training_dataset, test_dataset, \n",
    "#                                                                                     numInputs = 3, processInputs = MLFun.logFirstCol,\n",
    "#                                                                                     processTargets = MLFun.logAll)\n",
    "\n",
    "#Fuchs 2.91 scaling parameters: max = 1e-2, total = 1e5, avg = 1e-3, intens = 1e18\n",
    "#Fuchs 2.92 scaling parameters: max = 1e0, total = 1e8, avg = 1e-1\n",
    "\n",
    "maxScale = 1e0\n",
    "totalScale = 1e8\n",
    "avgScale = 1e-1\n",
    "intensScale = 1e19\n",
    "\n",
    "targetProcess = ThreeOutputRescaleLog(maxScale, totalScale, avgScale)\n",
    "inputProcess = MLFun.LogFirstColRescaleFun(intensScale)\n",
    "\n",
    "modelMSE, modelPercentError, trainMSE, trainPercent, timeList, cpuTime = MLFun.getModelError(DJINN, epochList, loss_function, \n",
    "                                                                                    training_dataset, test_dataset, \n",
    "                                                                                    numInputs = 3, processInputs = inputProcess,\n",
    "                                                                                    processTargets = targetProcess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitErrorList(errorList):\n",
    "    maxEnergyError = []\n",
    "    totalEnergyError = []\n",
    "    avgEnergyError = []\n",
    "\n",
    "    for element in errorList:\n",
    "        maxEnergyError.append(element[0])\n",
    "        totalEnergyError.append(element[1])\n",
    "        avgEnergyError.append(element[2])\n",
    "        \n",
    "    return maxEnergyError, totalEnergyError, avgEnergyError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model1LayerMSE)\n",
    "# print(model1LayerPercentError)\n",
    "# print('\\n')\n",
    "# print(trainMSE)\n",
    "# print(trainPercent)\n",
    "\n",
    "maxEnergyMSE, totalEnergyMSE, avgEnergyMSE = splitErrorList(modelMSE)\n",
    "maxEnergyPercent, totalEnergyPercent, avgEnergyPercent = splitErrorList(modelPercentError)\n",
    "\n",
    "trainMaxMSE, trainTotalMSE, trainAvgMSE = splitErrorList(trainMSE)\n",
    "trainMaxPercent, trainTotalPercent, trainAvgPercent = splitErrorList(trainPercent)\n",
    "\n",
    "#print(trainMaxMSE)\n",
    "#print(maxEnergyMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now plot errors and running time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Time spent plot\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "\n",
    "plt.plot(epochList, timeList, marker='s')\n",
    "plt.title(\"Number of epochs used vs. Time to train neural network\")\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.ylabel(\"Time in seconds\")\n",
    "\n",
    "\n",
    "#plt.legend(loc='upper left');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epochElement, timeElement in zip(epochList, timeList):\n",
    "    \n",
    "    minuteValue = timeElement / 60\n",
    "    \n",
    "    print(\"Number of epochs:\", epochElement)\n",
    "    print(\"Time spent:\", minuteValue, \"minutes\", '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CPU Time spent plot\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "\n",
    "plt.plot(epochList, cpuTime, marker='s')\n",
    "plt.title(\"Number of epochs used vs. CPU Time to train neural network\")\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.ylabel(\"Time in seconds\")\n",
    "\n",
    "\n",
    "#plt.legend(loc='upper left');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epochElement, timeElement in zip(epochList, cpuTime):\n",
    "    \n",
    "    minuteValue = timeElement / 60\n",
    "    \n",
    "    print(\"Number of epochs:\", epochElement)\n",
    "    print(\"CPU Time spent:\", minuteValue, \"minutes\", '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Percent Error plot\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "\n",
    "startIndex = 0\n",
    "\n",
    "plt.plot(epochList[startIndex:], maxEnergyPercent[startIndex:], c='b', marker=\"s\", label='Max Energy')\n",
    "plt.plot(epochList[startIndex:], totalEnergyPercent[startIndex:], c='r', marker=\"o\", label='Total Energy')\n",
    "plt.plot(epochList[startIndex:], avgEnergyPercent[startIndex:], c='g', marker='+', label='Average Energy')\n",
    "\n",
    "data = {'Epochs': epochList[startIndex:], \n",
    "        'Max Energy': maxEnergyPercent[startIndex:]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Write DataFrame to CSV\n",
    "df.to_csv('data_4hl'+str(batchSize) + str(numPoints)+'.csv', index=False)\n",
    "\n",
    "plt.title(\"Testing Data Relative Error\")\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.ylabel(\"Percent\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Percent Error plot for training data\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "\n",
    "plt.plot(epochList[startIndex:], trainMaxPercent[startIndex:], c='b', marker=\"s\", label='Max Energy')\n",
    "plt.plot(epochList[startIndex:], trainTotalPercent[startIndex:], c='r', marker=\"o\", label='Total Energy')\n",
    "plt.plot(epochList[startIndex:], trainAvgPercent[startIndex:], c='g', marker='+', label='Average Energy')\n",
    "plt.title(\"Training Data Relative Error\")\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.ylabel(\"Percent\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare errors of train and test using just the max energy % error\n",
    "\n",
    "fig = plt.figure()\n",
    "#ax1 = fig.add_subplot(1,1,1)\n",
    "\n",
    "plt.plot(epochList[startIndex:], trainMaxPercent[startIndex:], c='b', marker=\"s\", label='Training Data')\n",
    "plt.plot(epochList[startIndex:], maxEnergyPercent[startIndex:], c='r', marker=\"s\", label='Testing Data')\n",
    "\n",
    "plt.title(\"Testing vs. Training Data Error\")\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.ylabel(\"Max Energy Percent Error\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare errors of train and test using just the total energy % error\n",
    "\n",
    "fig = plt.figure()\n",
    "#ax1 = fig.add_subplot(1,1,1)\n",
    "\n",
    "plt.plot(epochList[startIndex:], trainTotalPercent[startIndex:], c='b', marker=\"s\", label='Training Data')\n",
    "plt.plot(epochList[startIndex:], totalEnergyPercent[startIndex:], c='r', marker=\"s\", label='Testing Data')\n",
    "\n",
    "plt.title(\"Testing vs. Training Data Error\")\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.ylabel(\"Total Energy Percent Error\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare errors of train and test using just the avg energy % error\n",
    "\n",
    "fig = plt.figure()\n",
    "#ax1 = fig.add_subplot(1,1,1)\n",
    "\n",
    "plt.plot(epochList[startIndex:], trainAvgPercent[startIndex:], c='b', marker=\"s\", label='Training Data')\n",
    "plt.plot(epochList[startIndex:], avgEnergyPercent[startIndex:], c='r', marker=\"s\", label='Testing Data')\n",
    "\n",
    "plt.title(\"Testing vs. Training Data Error\")\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.ylabel(\"Average Energy Percent Error\")\n",
    "plt.legend(loc='center right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch, maxError, totalError, avgError in zip(epochList, maxEnergyPercent, totalEnergyPercent, avgEnergyPercent):\n",
    "    print(\"Number of epochs:\", epoch)\n",
    "    print(\"Max energy percent error:\", maxError)\n",
    "    print(\"Total energy percent error:\", totalError)\n",
    "    print(\"Average energy percent error:\", avgError, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(epochList[startIndex:], maxEnergyMSE[startIndex:], marker = 's')\n",
    "plt.title(\"Max Energy Mean Squared Error\", pad = 20)\n",
    "plt.xlabel('Number of Epochs')\n",
    "plt.ylabel('Max Energy Error (MeV)')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(epochList[startIndex:], totalEnergyMSE[startIndex:], marker = 's')\n",
    "plt.title(\"Total Energy Mean Squared Error\", pad = 20)\n",
    "plt.xlabel('Number of Epochs')\n",
    "plt.ylabel('Total Energy Error (MeV)')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(epochList[startIndex:], avgEnergyMSE[startIndex:], marker = 's')\n",
    "plt.title(\"Average Energy Absolute Test Error\", pad = 20)\n",
    "plt.xlabel('Number of Epochs')\n",
    "plt.ylabel('Average Energy Error (MeV)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(epochList[startIndex:], trainMaxMSE[startIndex:], marker='s')\n",
    "plt.title(\"Max Energy Mean Squared Error\", pad = 20)\n",
    "plt.xlabel('Number of Epochs')\n",
    "plt.ylabel('Max Energy Error (MeV)')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(epochList[startIndex:], trainTotalMSE[startIndex:], marker='s')\n",
    "plt.title(\"Total Energy Mean Squared Error\", pad = 20)\n",
    "plt.xlabel('Number of Epochs')\n",
    "plt.ylabel('Total Energy Error (MeV)')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(epochList[startIndex:], trainAvgMSE[startIndex:], marker='s')\n",
    "plt.title(\"Average Energy Absolute Train Error\", pad = 20)\n",
    "plt.xlabel('Number of Epochs')\n",
    "plt.ylabel('Average Energy Error (MeV)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listSubtract(list1, list2):\n",
    "    result = []\n",
    "    \n",
    "    for x, y in zip(list1, list2):\n",
    "        difference = x - y\n",
    "        difference = abs(difference)\n",
    "        result.append(difference)\n",
    "        \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare train and test MSE errors on the max energy\n",
    "\n",
    "fig = plt.figure()\n",
    "#ax1 = fig.add_subplot(1,1,1)\n",
    "\n",
    "maxMSEDiff = listSubtract(trainMaxMSE, maxEnergyMSE)\n",
    "\n",
    "plt.plot(epochList[startIndex:], maxMSEDiff[startIndex:], c='b', marker=\"s\")\n",
    "\n",
    "plt.title(\"Max Energy Error MSE Difference Between Training and Testing\", pad = 20)\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.ylabel(\"Error Difference\")\n",
    "#plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare train and test percent errors on the max energy\n",
    "\n",
    "fig = plt.figure()\n",
    "#ax1 = fig.add_subplot(1,1,1)\n",
    "\n",
    "maxPercentDiff = listSubtract(trainMaxPercent, maxEnergyPercent)\n",
    "\n",
    "plt.plot(epochList[startIndex:], maxPercentDiff[startIndex:], c='b', marker=\"s\")\n",
    "\n",
    "plt.title(\"Max Energy Relative Error Difference Between Training and Testing\")\n",
    "plt.xlabel(\"Number of epochs\")\n",
    "plt.ylabel(\"Error Difference\")\n",
    "#plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch, maxError, totalError, avgError in zip(epochList, maxEnergyMSE, totalEnergyMSE, avgEnergyMSE):\n",
    "    print(\"Number of epochs:\", epoch)\n",
    "    print(\"Max energy MSE:\", maxError)\n",
    "    print(\"Total energy MSE:\", totalError)\n",
    "    print(\"Average energy MSE:\", avgError, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (Conda 2022.05) [python/3.9-2022.05]",
   "language": "python",
   "name": "python39_202205"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
